---
title: "The Difference Between K-means Clustering and Spherical K-means Clustering Illuminated by using Plant Metabolites data"
author: "Amit Sengupta，Danielle Totten， Aixin Zhang"
date: "11/14/2018"
output: html_document
---

```{r setup, include=FALSE}
library(data.table)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readxl)
library(reshape)
library(data.table)
library(lsa)
library(stats)
library(ggrepel)
```

## Introduction
In this tutorial, we will discuss the difference between classic K-means clustering and spherical k-means clustering by using real world plant metabolites data.The dataset containing sixty-five plant metabolites to see how they change in diluted Farnesene solution at 24 hours and 48 hours. Similar metabolites are grouped based on their change tedency by using the K-means clustering and spherical K-means clustering. The differences in two methods and some visualization ideas are discussed as well.

##K-Mean Algorithm 
K-mean clustering is popular and basic clustering method in machine learning, it aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean so as to minimize the within-cluster variance[1]. The distance matrix used for K-Mean clustering is the Euclidean distance, we seek to minimize a Euclidean distance between the cluster center and the members of the cluster[2], and the variance is used as a measure of cluster scatter.

The K-mean clustering starts with the choice of K centroids which can be randomly chosen from the dataset. In the first step, each data-point is assigned to its nearest centroid based on the Euclidian distance. In the second step, the centroids are recomputed by taking the mean of all data points assigned to the centroid. The algorithm iterates between the two steps until no data-point changes a cluster. The algorithm is guaranteed to converge to an optimum value, presumably a local optimum. One common method of choosing K is the elbow method where the average within cluster distance to the centroid vs K starts showing the least change.

##Sperical K-Means Algorithm
Compare with K-means algorithm, the sperical K-means clustering use the cosine dissimilarity as the distance matrix to cluster. We treat each data-point as a vector, when two vectors with a opposite orientation then they have a cosine dissimilarity of -1, when two vectors are at ninety degree  then they have a dissimilarity of 0. In this way, it tends to capture better the change tendency (slope) of each matebolitics.
The cosine dissimilarity is calculated as followed formula:
d(A, B ) = 1 − cos(A, B ) = 1- (A ∙B)/(||A||*||B||)

In the first step of sperical K-means, each point is assigned to a centroid based on maximum cosine dissimilarity. In the second step, the centroids are recomputed by using the average of the points in each cluster. These two steps are repeated until no data point changes a cluster.The common method of choosing K is also by checking the elbow plot where the average within cluster distance to the centroid vs K starts showing the least change.

##Before going any further, Let's look at the plot without clustering
```{r, echo=FALSE}

plant <- read.csv("~/Desktop/plant.csv")
plant$Benzoic.acid<-NULL
plant$sample_group <- as.factor(plant$sample_group)
plant$time_in_hour <- as.factor(plant$time_in_hour)
plant <- dplyr::select(plant, -replication)
plant <- data.table(dplyr::filter(plant, sample_group=="f"))
plant.melt <- melt.data.table(plant,id.vars = c('sample_group', 'time_in_hour'),variable.name = 'metabolism_group')
p <- data.frame(cast(plant.melt, metabolism_group ~ time_in_hour + sample_group, mean))
p1<-p
p2 <- p[,-1]
rownames(p2) <- p[,1]
p2 <- data.frame(p2)
p_scaled <- scale(t(p2))
p_not_scale<-t(p2)
```

```{r pressure, echo=FALSE}
qplot(p2$X24_f, p2$X48_f)
```


By Checking the plot of data, we can approximately see that how much of a difference is there between 24 and 48 hrs. 

##Let's do the K-Means
Using elbow plots to determine the cluster number, let's check both between and within
```{r, echo=FALSE}
set.seed(123)
k.max <- 15

wss <- sapply(1:k.max, 
              function(k){kmeans(p2, k, nstart=1, iter.max=15)$tot.withinss})
plot(1:k.max, wss, type="b")
bss <- sapply(1:k.max, 
              function(k){kmeans(p2, k, nstart=1, iter.max=15)$betweenss})
plot(1:k.max, bss, type="b")

```


The elbow point is around 5, so let's do 5 clusters.

The result of 5 clusters under K-means is visialized as followed:

```{r}
set.seed(123)
p.clust5 <- kmeans(p2, 5)
plot(p2, col=(p.clust5$cluster +1), main="K-Means Clustering Results with K=5", xlab="", ylab="", pch=20, cex=2)
clust5 <- data.frame(p.clust5$cluster)
p1$cluster_k <- p.clust5$cluster

```


##Let's do the Spherical K-Means
Using elbow plots to determine the cluster number, let's check both between and within
```{r, echo=FALSE}
plant.matrix.cosine <- 1-cosine(t(p2))
set.seed(123)
k.max <- 15

wss <- sapply(1:k.max, 
              function(k){kmeans(plant.matrix.cosine, k, nstart=1, iter.max=15)$tot.withinss})
plot(1:k.max, wss, type="b")
bss <- sapply(1:k.max, 
              function(k){kmeans(plant.matrix.cosine, k, nstart=1, iter.max=15)$betweenss})
plot(1:k.max, bss, type="b")

```

The elbow point indicate that 6 clusters maybe a good strat. However, we will choose 5 clusters for the consistent with K-means clustering

```{r}
set.seed(123)
p.sk.clust5 <-kmeans(plant.matrix.cosine, 5)
```

The result of 5 clusters under Sperical K-means is visialized as followed:
```{r, echo = FALSE}
p$cluster <- p.sk.clust5$cluster
p<-data.table(p)
p[, Ave_24h_normalized:= X24_f/sqrt(X24_f^2 + X48_f^2)]
p[, Ave_48h_normalized:=X48_f/sqrt(X24_f^2 + X48_f^2)]

ggplot(p)+
  geom_label_repel(aes(label = metabolism_group, x=Ave_24h_normalized, y=Ave_48h_normalized, color = factor(cluster)), 
                   size=2)+
  geom_segment(aes(x=0, y=0, 
                   xend=Ave_24h_normalized, yend=Ave_48h_normalized, color=factor(cluster)))+
  xlab('Ave_24h normalized')+ylab('Ave_48h normalized') + 
  scale_color_discrete(name = 'Cluster')
```

##Result
```{r, echo=FALSE}
result<-merge(p, p1, by = "metabolism_group")
result<-result %>% select("metabolism_group", "cluster", "cluster_k")
```
From the table we can tell that classical k-means clustering and spherical k-means gave the different clusters based on their algorithm.Spherical k-means more focus on the change tendency but classical k-means focus the change amount. However, 
If you normalize vectors to the unit circle then euclidean distance, the classical K-means works perfectly well because in the unit circle the euclidean distance works as well as the angular distance. Thus, the method selection is mainly based on the research question. Moreover, spherical k-means is widely used in text data. Let’s imagine our vocabulary consists only of two letter: A and Z, we can treat the text as a point. If we want cluster the text based on the struction, then AA is more similar to ZZ compare with AB, we use (1,1) for AA, (26,26) for ZZ, (1,2)for AB, if we use the standard euclidean distance then it will cluster (1,1) and (1,2) together but not  with (26,26). However, if we use cosine dissimilarity, then (1,1) will have same angle with (26,26), thus AA and ZZ will group together. Back to our research question, we want to group the similar metabolisms based on their change tendency, which mean if one metabolisms changed from 0.5 to 1 and another one changed from 1 to 2, then we hope we can catch such multiple grouth change tendency and group them together. Thus, Spherical k-means is better than k-means for this research.






